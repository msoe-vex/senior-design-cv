{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Localization Demonstration Notebook\n",
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyrealsense2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24008/2431893333.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyrealsense2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdetect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyrealsense2'"
     ]
    }
   ],
   "source": [
    "import pyrealsense2 as rs\n",
    "import cv2\n",
    "import detect\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring constant variables, obtained from values in Intel and VRC documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal_length = ((448.0-172.0) * 24.0) / 11.0\n",
    "# ['Blue Goal', 'Blue Platform', 'Blue Robot', 'Neutral Goal', 'Red Goal', 'Red Platform', 'Red Robot', 'Ring']\n",
    "width_dict = {\"7\":3.5, \"3\":12.5, \"0\":12.5, \"4\":12.5, \"2\":5.5, \"5\":5.5, \"1\":53.0, \"6\":53.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting image pipeline from D435 camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyrealsense2.pyrealsense2.pipeline_profile at 0x225ee2459b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "pipeline.start(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object localization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_distance(cls, x, y, w, h, depth_frame):\n",
    "    # calculating distance using trigonometric properties\n",
    "    trig_distance = (width_dict[str(cls)] * focal_length)/w\n",
    "    \n",
    "    # extract average distance from depth map and convert to inches\n",
    "    depth_distance_meters = (depth_frame.get_distance(x, y) +\\\n",
    "                             depth_frame.get_distance(x+2, y) +\\\n",
    "                             depth_frame.get_distance(x, y+2) +\\\n",
    "                             depth_frame.get_distance(x-2, y) +\\\n",
    "                             depth_frame.get_distance(x, y-2))/5.0\n",
    "    depth_distance = 39.3701 * depth_distance_meters\n",
    "    \n",
    "    # weighting and combining localization methods\n",
    "    distance = (trig_distance * .2) + (depth_distance_meters * .8) \n",
    "    \n",
    "    # in the event that depthmap can't detect distance, only use trig distance\n",
    "    if (depth_distance_meters == 0):\n",
    "        distance = trig_distance\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Localization Fusion Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_height = 10\n",
    "cam_angle = -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_lf(obj_x, obj_y, obj_distance, robot_coords):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return (calculated_x, calculated_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo x, y, theta\n",
    "robot_coords = (30, 30, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using identification and localization to process images from the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.45, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  d8f4ccd torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 Done. (1.709s)\n",
      "Speed: 10.3ms pre-process, 1709.2ms inference, 1.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "101 labels saved to runs\\detect\\yolo_obj\\labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3: 21.436225156173514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.45, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  d8f4ccd torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 1 class3, Done. (1.356s)\n",
      "Speed: 0.0ms pre-process, 1356.0ms inference, 49.6ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "101 labels saved to runs\\detect\\yolo_obj\\labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5: 8.414897786246405\n",
      "7: 22.436205949646435\n",
      "7: 18.70843643535267\n",
      "3: 22.71331083546177\n",
      "7: 13.64565396085163\n",
      "7: 12.631620980915539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.45, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  d8f4ccd torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 1 class3, 1 class5, 4 class7s, Done. (1.376s)\n",
      "Speed: 0.0ms pre-process, 1376.1ms inference, 38.5ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "101 labels saved to runs\\detect\\yolo_obj\\labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 121.40762463343108\n",
      "7: 110.92822966507177\n",
      "7: 30.545454545454543\n",
      "7: 75.27272727272727\n",
      "7: 43.90909090909091\n",
      "3: 8.926227760828283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.45, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  d8f4ccd torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 1 class0, 1 class3, 4 class7s, Done. (1.410s)\n",
      "Speed: 4.4ms pre-process, 1410.4ms inference, 7.4ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "101 labels saved to runs\\detect\\yolo_obj\\labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 112.34735413839891\n",
      "7: 30.99465240641711\n",
      "3: 9.961172200524233\n",
      "7: 43.01298701298701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.45, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  d8f4ccd torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 1 class0, 1 class3, 2 class7s, Done. (1.388s)\n",
      "Speed: 5.7ms pre-process, 1388.3ms inference, 2.7ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "101 labels saved to runs\\detect\\yolo_obj\\labels\n",
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.45, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  d8f4ccd torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 Done. (1.369s)\n",
      "Speed: 5.9ms pre-process, 1369.3ms inference, 7.1ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "101 labels saved to runs\\detect\\yolo_obj\\labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7: 25.278821886236017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.45, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  d8f4ccd torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 1 class7, Done. (1.371s)\n",
      "Speed: 1.1ms pre-process, 1370.7ms inference, 3.1ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "101 labels saved to runs\\detect\\yolo_obj\\labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7: 4.6699913239968485\n",
      "7: 40.531468531468526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.45, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  d8f4ccd torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 2 class7s, Done. (1.410s)\n",
      "Speed: 2.8ms pre-process, 1409.8ms inference, 5.6ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "101 labels saved to runs\\detect\\yolo_obj\\labels\n",
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.45, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  d8f4ccd torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 Done. (1.409s)\n",
      "Speed: 0.0ms pre-process, 1409.1ms inference, 1.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "101 labels saved to runs\\detect\\yolo_obj\\labels\n",
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.45, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  d8f4ccd torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 Done. (1.381s)\n",
      "Speed: 3.4ms pre-process, 1381.2ms inference, 2.4ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "101 labels saved to runs\\detect\\yolo_obj\\labels\n",
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.45, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  d8f4ccd torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 Done. (1.393s)\n",
      "Speed: 2.4ms pre-process, 1393.3ms inference, 2.3ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "101 labels saved to runs\\detect\\yolo_obj\\labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: 37.008800005912775\n",
      "0: 35.2583960640332\n",
      "2: 33.33040000915527\n",
      "4: 20.557580359620605\n",
      "2: 35.07355790389211\n",
      "2: 60.3429818279093\n",
      "3: 25.72657874567299\n",
      "0: 23.647207277037882\n",
      "2: 23.050979329799784\n",
      "7: 28.31509819204157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.45, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  d8f4ccd torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 2 class0s, 5 class2s, 1 class3, 1 class4, 1 class7, Done. (1.415s)\n",
      "Speed: 4.7ms pre-process, 1414.5ms inference, 0.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "101 labels saved to runs\\detect\\yolo_obj\\labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4: 24.504724937939574\n",
      "2: 33.336960020065305\n",
      "2: 23.0477793233148\n",
      "2: 39.173505888265716\n",
      "7: 35.34807274558327\n",
      "2: 26.700000020027158\n",
      "2: 33.33360001564026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.45, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  d8f4ccd torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 5 class2s, 1 class4, 1 class7, Done. (1.385s)\n",
      "Speed: 2.7ms pre-process, 1384.9ms inference, 7.4ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "101 labels saved to runs\\detect\\yolo_obj\\labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: 41.60640001296997\n",
      "2: 37.00240001678466\n",
      "2: 24.733973344167072\n",
      "3: 27.086316889601868\n",
      "0: 29.463848976988892\n",
      "2: 236.57142857142853\n",
      "2: 35.06267790091665\n",
      "0: 22.66643083343998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.45, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  d8f4ccd torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 2 class0s, 5 class2s, 1 class3, Done. (1.506s)\n",
      "Speed: 6.3ms pre-process, 1506.4ms inference, 12.8ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "101 labels saved to runs\\detect\\yolo_obj\\labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4: 29.788156589518255\n",
      "0: 32.286787791040005\n",
      "2: 30.370370921221646\n",
      "2: 51.21544617139376\n",
      "7: 38.59010116900294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.45, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  d8f4ccd torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 1 class0, 2 class2s, 1 class4, 1 class7, Done. (1.340s)\n",
      "Speed: 2.9ms pre-process, 1339.5ms inference, 3.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "101 labels saved to runs\\detect\\yolo_obj\\labels\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # extracting data from the image pipeline\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "    # IO for trained YOLOv5 model (color_img->game_objects)\n",
    "    cv2.imwrite('img.png', color_image)\n",
    "\n",
    "    !python detect.py --source img.png --weights static/best_torchscript.pt --conf 0.45 --name yolo_obj --view-img --save-txt --nosave --exist-ok\n",
    "\n",
    "    # reading labels from localization output\n",
    "    game_objects = []\n",
    "    with open('runs/detect/yolo_obj/labels/img.txt', 'r+') as f:\n",
    "        game_objects = f.readlines()\n",
    "        f.truncate(0)\n",
    "        f.close()\n",
    "\n",
    "    # calculating distance for all game objects in frame\n",
    "    for obj in game_objects:\n",
    "        obj_array = obj.split()\n",
    "        cls, x, y, w, h = int(obj_array[0]), int(round(float(obj_array[1]))), int(round(float(obj_array[2]))), float(obj_array[3]), float(obj_array[4])\n",
    "#         print(f'{obj.split()[0]}: {obj_distance(obj, depth_frame)}')\n",
    "        obj_distance = obj_distance(cls, x, y, w, h, depth_frame)\n",
    "        obj_location = obj_lf(x, y, obj_distance, robot_coords)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88428834693a6c333f2ad28ddc580536482abcdcaaa68d12bbcf87185ab43770"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('robotics': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
