{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Localization Demonstration Notebook\n",
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrealsense2 as rs\n",
    "import cv2\n",
    "import detect\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring constant variables, obtained from values in Intel and VRC documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal_length = ((448.0-172.0) * 24.0) / 11.0\n",
    "# ['Blue Goal', 'Blue Platform', 'Blue Robot', 'Neutral Goal', 'Red Goal', 'Red Platform', 'Red Robot', 'Ring']\n",
    "width_dict = {\"7\":3.5, \"3\":12.5, \"0\":12.5, \"4\":12.5, \"2\":5.5, \"5\":5.5, \"1\":53.0, \"6\":53.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting image pipeline from D435 camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyrealsense2.pyrealsense2.pipeline_profile at 0x201496a59b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "pipeline.start(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object localization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_distance(obj, depth_frame):\n",
    "    # file parsing\n",
    "    obj_array = obj.split()\n",
    "    x, y, w, h = int(round(float(obj_array[1]))), int(round(float(obj_array[2]))), float(obj_array[3]), float(obj_array[4])\n",
    "\n",
    "    # calculating distance using trigonometric properties\n",
    "    trig_distance = (width_dict[str(obj_array[0])] * focal_length)/w\n",
    "    \n",
    "    # extract average distance from depth map and convert to inches\n",
    "    depth_distance_meters = (depth_frame.get_distance(x, y) +\\\n",
    "                             depth_frame.get_distance(x+2, y) +\\\n",
    "                             depth_frame.get_distance(x, y+2) +\\\n",
    "                             depth_frame.get_distance(x-2, y) +\\\n",
    "                             depth_frame.get_distance(x, y-2))/5.0\n",
    "    depth_distance = 39.3701 * depth_distance_meters\n",
    "    \n",
    "    # weighting and combining localization methods\n",
    "    distance = (trig_distance * .2) + (depth_distance_meters * .8) \n",
    "    \n",
    "    # in the event that depthmap can't detect distance, only use trig distance\n",
    "    if (depth_distance_meters == 0):\n",
    "        distance = trig_distance\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using identification and localization to process images from the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3: 112.34735413839891\n",
      "3: 119.48051948051948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.6, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  a0dc65c torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 2 class3s, Done. (1.435s)\n",
      "Speed: 2.0ms pre-process, 1434.8ms inference, 5.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "1 labels saved to runs\\detect\\yolo_obj\\labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7: 58.54545454545454\n",
      "7: 95.80165289256198\n",
      "7: 20.498167288520122\n",
      "7: 67.9882697947214\n",
      "7: 9.429305407083241\n",
      "7: 55.464114832535884\n",
      "7: 95.80165289256198\n",
      "7: 49.014799154334035\n",
      "3: 34.371108343711086\n",
      "7: 67.9882697947214\n",
      "7: 87.81818181818181\n",
      "3: 61.69895678092399\n",
      "7: 84.30545454545454\n",
      "7: 63.86776859504132\n",
      "7: 16.936770912257106\n",
      "7: 11.66530298703025\n",
      "3: 76.80890538033395\n",
      "7: 8.229580637035948\n",
      "7: 50.18181818181818\n",
      "7: 47.90082644628099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.6, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  a0dc65c torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 3 class3s, 17 class7s, Done. (1.063s)\n",
      "Speed: 3.0ms pre-process, 1063.3ms inference, 2.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "1 labels saved to runs\\detect\\yolo_obj\\labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7: 11.86557091626254\n",
      "7: 13.755253968385658\n",
      "7: 20.234327278353952\n",
      "3: 17.872029950529495\n",
      "7: 9.938479838477129\n",
      "3: 10.25812178453052\n",
      "7: 12.204436367641796\n",
      "7: 14.552509108456698\n",
      "7: 9.796872746727683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.6, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  a0dc65c torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 2 class3s, 7 class7s, Done. (1.045s)\n",
      "Speed: 2.9ms pre-process, 1045.1ms inference, 2.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "1 labels saved to runs\\detect\\yolo_obj\\labels\n",
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.6, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  a0dc65c torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 Done. (1.015s)\n",
      "Speed: 3.0ms pre-process, 1015.5ms inference, 2.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "1 labels saved to runs\\detect\\yolo_obj\\labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7: 63.86776859504132\n",
      "7: 61.98930481283422\n",
      "3: 203.43980343980343\n",
      "7: 7.524175319626001\n",
      "7: 36.338557993730404\n",
      "7: 31.457259158751693\n",
      "7: 31.457259158751693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.6, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  a0dc65c torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 1 class3, 6 class7s, Done. (1.469s)\n",
      "Speed: 3.0ms pre-process, 1469.4ms inference, 6.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "1 labels saved to runs\\detect\\yolo_obj\\labels\n",
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.6, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  a0dc65c torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 Done. (1.695s)\n",
      "Speed: 4.8ms pre-process, 1695.5ms inference, 2.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "1 labels saved to runs\\detect\\yolo_obj\\labels\n",
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.6, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  a0dc65c torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 Done. (2.892s)\n",
      "Speed: 7.0ms pre-process, 2892.0ms inference, 2.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "1 labels saved to runs\\detect\\yolo_obj\\labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7: 9.369876373897897\n",
      "7: 8.47644064056682\n",
      "7: 8.642705464883283\n",
      "7: 8.47548065292644\n",
      "7: 8.015020614681822\n",
      "7: 8.33702478781777\n",
      "7: 7.563410060644517\n",
      "7: 8.104612244298636\n",
      "3: 11.910030286438118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.6, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  a0dc65c torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 1 class3, 8 class7s, Done. (1.853s)\n",
      "Speed: 4.0ms pre-process, 1852.9ms inference, 6.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "1 labels saved to runs\\detect\\yolo_obj\\labels\n",
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['static/best_torchscript.pt'], source=img.png, imgsz=[640, 640], conf_thres=0.6, iou_thres=0.45, max_det=1000, device=, view_img=True, save_txt=True, save_conf=False, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=yolo_obj, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  a0dc65c torch 1.10.0 CPU\n",
      "\n",
      "Loading static/best_torchscript.pt for TorchScript inference...\n",
      "image 1/1 C:\\Users\\wellerj\\Documents\\github\\senior-design-cv\\localization\\img.png: 640x640 Done. (1.015s)\n",
      "Speed: 2.0ms pre-process, 1014.9ms inference, 1.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\yolo_obj\u001b[0m\n",
      "1 labels saved to runs\\detect\\yolo_obj\\labels\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Frame didn't arrive within 5000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12228/3634900817.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# extracting data from the image pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mdepth_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_depth_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcolor_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_color_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Frame didn't arrive within 5000"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # extracting data from the image pipeline\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "    # IO for trained YOLOv5 model (color_img->game_objects)\n",
    "    cv2.imwrite('img.png', color_image)\n",
    "\n",
    "    !python detect.py --source img.png --weights static/best_torchscript.pt --conf 0.60 --name yolo_obj --view-img --save-txt --nosave --exist-ok\n",
    "\n",
    "    # reading labels from localization output\n",
    "    game_objects = []\n",
    "    with open('runs/detect/yolo_obj/labels/img.txt', 'r+') as f:\n",
    "        game_objects = f.readlines()\n",
    "        f.truncate(0)\n",
    "        f.close()\n",
    "\n",
    "    # calculating distance for all game objects in frame\n",
    "    for obj in game_objects:\n",
    "        print(f'{obj.split()[0]}: {obj_distance(obj, depth_frame)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
